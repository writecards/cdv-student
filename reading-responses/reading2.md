From this week’s materials it is clear that automated decision making systems are full of biases that have drastic effects on individuals’ livelihoods. As Joy Buolamwini, Trevor Paglen, Kate Crawford, and Virginia Eubanks all elude, these systems of surveillance, data collection, and algorithmic policing are not “objective” in the way one may assume a machine might be—in reality, these highly intelligent systems are a cocktail of politics and human bias, which are ultimately controlled by those who hold power. The people/institutions with power use automated systems as a mask, or “empathy-overwrite,” to try to justify lack of resources for those in need. It is an attempt to shift the blame onto individuals for being poor, when it is actually the fault of racist/ableist/sexist/classist narratives embedded in the categorization process of who deserves help and who doesn’t. 

I would like to bring up an example case from both the U.S. and China. In the November elections in California (where I voted for the first time) there was a proposition that called for replacing criminal suspects’ cash bail with an automated risk assessment system. This would introduce an algorithm to determine who is high risk/likely to avoid court dates, thus needing to stay in jail, and who is low risk and can be let out, instead of a cash based bail system. While the bail system (and prison system overall) is already messed up, the proposition would bring in yet another form of racialized violence in the form of algorithmic oppression. It didn’t end up passing, but the results were a little close (43.59% Californians voted yes, 56.41% no). 

As for a case in China, the social credit system is a highly technical mechanism that combines data from a variety of sources to place citizens on a scale. The blacklist portion of the system would prevent people from accessing certain resources and limit their movements. This is problematic in that it seems once someone is blacklisted or categorized in a certain way it’s hard to get out of that box. All in all, I believe that the desire to classify people into boxes (regardless of political context) is a colonial desire… and I believe change *starts* with bringing more representation to the table and taking a closer look at the intentions of the automated systems at hand. 
