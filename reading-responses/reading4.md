An example of a predictive system is Eleme delivery time predictions - the system compiles information like time of day, traffic conditions, weather, and expected preparation time for the food in order to estimate how long the customer will have to wait for food. Another example is insurance fraud prediction algorithms as mentioned in Virginia Eubanks’s Automating Inequality. This algorithm looks at data on timing of account activity to single out potential suspects of insurance fraud, which is not necessarily always correct. A third example is the use of “social determinants” in healthcare, where behavioral data, income, zip code among other factors are compiled into a system to predict when a patient might cancel an appointment. To be more specific, the example I found is The Midwest Institute for Minimally Invasive Therapies (MIMIT), they are utilizing publicly available data from LinkedIn and Facebook in electronic health records, in conjunction with social determinants of health to predict when a patient will skip an appointment, which allows administrators to prepare a list of patients to call instead. 

In all of these examples, the purpose of relying on prediction is to promote efficiency and increase profit. From an operational standpoint, it makes sense that a food delivery company, an insurance company, or a healthcare provider would want to use predictive analytics to improve the company. However, with these systems in place there is no doubt that there are social implications that are being swept under the table. As Virginia Eubanks notes, once a system categorizes you in a certain way, it is hard to escape that box. The harm is especially evident in her example of the suspected insurance fraud, and extends to poverty management systems throughout nation states. 

Not super closely related, but throughout this week’s materials I was repeatedly reminded of Alex Garland’s sci-fi TV series called Devs, which involves a quantum computer machine that is able to predict the future. The TV series along with the article by Hannah Fry make me wonder about predictability and free will. I agree with Fry that from a statistical standpoint we can make predictions about general populations, and that the problem comes with predicticting an individual’s behavior. I end with a rather speculative, philosophical question, which is that if we were really able to use artificial intelligence to predict individual behavior (and not just generalizations), would people’s behavior actually change?
